var documenterSearchIndex = {"docs":
[{"location":"_index/","page":"-","title":"-","text":"CurrentModule = AirspeedVelocity","category":"page"},{"location":"_index/","page":"-","title":"-","text":"<README>","category":"page"},{"location":"_index/","page":"-","title":"-","text":"Pages = [\"api.md\"]","category":"page"},{"location":"_index/","page":"-","title":"-","text":"Modules = [AirspeedVelocity]","category":"page"},{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"benchmark(package_name::String, rev::Vector{String}; output_dir::String=\".\", script::Union{String,Nothing}=nothing, tune::Bool=false, exeflags::Cmd=``, extra_pkgs::Vector{String}=String[])","category":"page"},{"location":"api/#AirspeedVelocity.Utils.benchmark-Tuple{String, Vector{String}}","page":"API","title":"AirspeedVelocity.Utils.benchmark","text":"benchmark(package_name::String, rev::Union{String,Vector{String}}; output_dir::String=\".\", script::Union{String,Nothing}=nothing, tune::Bool=false, exeflags::Cmd=``, extra_pkgs::Vector{String}=String[])\n\nRun benchmarks for a given Julia package.\n\nThis function runs the benchmarks specified in the script for the package defined by the package_spec. If script is not provided, the function will use the default benchmark script located at {PACKAGE_SRC_DIR}/benchmark/benchmarks.jl.\n\nThe benchmarks are run using the SUITE variable defined in the benchmark script, which should be of type BenchmarkTools.BenchmarkGroup. The benchmarks can be run with or without tuning depending on the value of the tune argument.\n\nThe results of the benchmarks are saved to a JSON file named results_packagename@rev.json in the specified output_dir.\n\nArguments\n\npackage_name::String: The name of the package for which to run the benchmarks.\nrev::Union{String,Vector{String}}: The revision of the package for which to run the benchmarks. You can also pass a vector of revisions to run benchmarks for multiple versions of a package.\noutput_dir::String=\".\": The directory where the benchmark results JSON file will be saved (default: current directory).\nscript::Union{String,Nothing}=nothing: The path to the benchmark script file. If not provided, the default script at {PACKAGE}/benchmark/benchmarks.jl will be used.\ntune::Bool=false: Whether to run benchmarks with tuning (default: false).\nexeflags::Cmd=``: Additional execution flags for running the benchmark script (default: empty).\nextra_pkgs::Vector{String}=String[]: Additional packages to add to the benchmark environment.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"benchmark(package_specs::Vector{PackageSpec}; output_dir::String = \".\", script::Union{String,Nothing} = nothing, tune::Bool = false, exeflags::Cmd = ``, extra_pkgs = String[])","category":"page"},{"location":"api/#AirspeedVelocity.Utils.benchmark-Tuple{Vector{Pkg.Types.PackageSpec}}","page":"API","title":"AirspeedVelocity.Utils.benchmark","text":"benchmark(package_specs::Union{PackageSpec,Vector{PackageSpec}}; output_dir::String=\".\", script::Union{String,Nothing}=nothing, tune::Bool=false, exeflags::Cmd=``, extra_pkgs::Vector{String}=String[])\n\nRun benchmarks for a given Julia package.\n\nThis function runs the benchmarks specified in the script for the package defined by the package_spec. If script is not provided, the function will use the default benchmark script located at {PACKAGE_SRC_DIR}/benchmark/benchmarks.jl.\n\nThe benchmarks are run using the SUITE variable defined in the benchmark script, which should be of type BenchmarkTools.BenchmarkGroup. The benchmarks can be run with or without tuning depending on the value of the tune argument.\n\nThe results of the benchmarks are saved to a JSON file named results_packagename@rev.json in the specified output_dir.\n\nArguments\n\npackage::Union{PackageSpec,Vector{PackageSpec}}: The package specification containing information about the package for which to run the benchmarks. You can also pass a vector of package specifications to run benchmarks for multiple versions of a package.\noutput_dir::String=\".\": The directory where the benchmark results JSON file will be saved (default: current directory).\nscript::Union{String,Nothing}=nothing: The path to the benchmark script file. If not provided, the default script at {PACKAGE}/benchmark/benchmarks.jl will be used.\ntune::Bool=false: Whether to run benchmarks with tuning (default: false).\nexeflags::Cmd=``: Additional execution flags for running the benchmark script (default: empty).\nextra_pkgs::Vector{String}=String[]: Additional packages to add to the benchmark environment.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = AirspeedVelocity","category":"page"},{"location":"#AirspeedVelocity","page":"Home","title":"AirspeedVelocity","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"","page":"Home","title":"Home","text":"AirspeedVelocity.jl strives to make it easy to benchmark Julia packages over their lifetime. It is inspired by asv.","category":"page"},{"location":"#Motivation","page":"Home","title":"Motivation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Why not PkgBenchmark.jl?","category":"page"},{"location":"","page":"Home","title":"Home","text":"PkgBenchmark.jl is a thin wrapper of BenchmarkTools and Git, which might be enough for most users. However, for me it was a bit too thin â€“ this package tries to do more, and do it automatically (including plot generation, similar to asv), especially for common workflows.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package allows you to:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Generate benchmarks directly from the terminal with an easy-to-use CLI\nQuery many commits/tags/branches at a time, rather than requiring separate calls for each revision\nPlot those benchmarks, automatically flattening your benchmark suite into a list of plots with generated titles, with the x-axis showing revisions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package also freezes the benchmark script, so there is no worry about the old history overwriting the benchmark.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install the CLI with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia -e 'using Pkg; \\\n          Pkg.add(url=\"https://github.com/MilesCranmer/AirspeedVelocity.jl.git\"); \\\n          Pkg.build(\"AirspeedVelocity\")'","category":"page"},{"location":"","page":"Home","title":"Home","text":"This will install two executables at ~/.julia/bin - make sure to have it on your PATH.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You may then use the CLI with, e.g.,","category":"page"},{"location":"","page":"Home","title":"Home","text":"benchpkg Transducers --rev='v0.4.65,v0.4.70,master' --add='BangBang,ArgCheck,Referenceables,SplitApplyCombine'","category":"page"},{"location":"","page":"Home","title":"Home","text":"which will download benchmark/benchmarks.jl of Transducers.jl, run the benchmarks for all revisions given (v0.4.65, v0.4.70, and master), and then save the JSON results in the current directory. Here, we also specify BangBang.jl, ArgCheck.jl, Referenceables.jl, and SplitApplyCombine.jl as additional packages used explicitly inside the benchmarks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can generate plots of the revisions with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"benchpkgplot Transducers --rev='v0.4.65,v0.4.70,master' --npart=10","category":"page"},{"location":"","page":"Home","title":"Home","text":"which will generate a png file of plots, showing the change with each revision. The --npart flag specifies the maximum number of plots per page; if there are more than npart plots, they will be split into multiple images.","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also provide a custom benchmark. For example, let's say you have a file script.jl, defining a benchmark for SymbolicRegression.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using BenchmarkTools, SymbolicRegression\nconst SUITE = BenchmarkGroup()\nSUITE[\"eval_tree_array\"] = begin\n    b = BenchmarkGroup()\n    options = Options(; binary_operators=[+, -, *], unary_operators=[cos])\n    tree = Node(; feature=1) + cos(3.2f0 * Node(; feature=2))\n    X = randn(Float32, 2, 10)\n    f() = eval_tree_array(tree, X, options)\n    b[\"10\"] = @benchmarkable f() evals=1 samples=100\n\n    X2 = randn(Float32, 2, 20)\n    f2() = eval_tree_array(tree, X2, options)\n    b[\"20\"] = @benchmarkable f2() evals=1 samples=100\n    b\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"we can run this benchmark over the history of SymbolicRegression.jl with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"benchpkg SymbolicRegression -r v0.15.3,v0.16.2 -s script.jl -o results/ --exeflags=\"--threads=4 -O3\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"where we have also specified the output directory and extra flags to pass to the julia executable. We can also now visualize this:","category":"page"},{"location":"","page":"Home","title":"Home","text":"benchpkgplot SymbolicRegression -r v0.15.3,v0.16.2 -i results/ -o plots/ --format=pdf","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The CLI is documented as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"    benchpkg package_name [-r --rev <arg>] [-o, --output_dir <arg>]\n                          [-s, --script <arg>] [-e, --exeflags <arg>]\n                          [-a, --add <arg>] [-t, --tune]\n                          [-u, --url <arg>]\n\nBenchmark a package over a set of revisions.\n\n# Arguments\n\n- `package_name`: Name of the package.\n\n# Options\n\n- `-r, --rev <arg>`: Revisions to test (delimit by comma).\n- `-o, --output_dir <arg>`: Where to save the JSON results.\n- `-s, --script <arg>`: The benchmark script. Default: `{PACKAGE_SRC_DIR}/benchmark/benchmarks.jl`.\n- `-e, --exeflags <arg>`: CLI flags for Julia (default: none).\n- `-a, --add <arg>`: Extra packages needed (delimit by comma).\n- `-u, --url <arg>`: URL of the package.\n\n# Flags\n\n- `-t, --tune`: Whether to run benchmarks with tuning (default: false).\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"For plotting, you can use the benchpkgplot function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"    benchpkgplot package_name [-r --rev <arg>] [-i --input_dir <arg>]\n                              [-o --output_dir <arg>] [-n --npart <arg>]\n                              [-f --format <arg>]\n\nPlot the benchmarks of a package as created with `benchpkg`.\n\n# Arguments\n\n- `package_name`: Name of the package.\n\n# Options\n\n- `-r, --rev <arg>`: Revisions to test (delimit by comma).\n- `-i, --input_dir <arg>`: Where the JSON results were saved (default: \".\").\n- `-o, --output_dir <arg>`: Where to save the plots results (default: \".\").\n- `-n, --npart <arg>`: Max number of plots per page (default: 10).\n- `-f, --format <arg>`: File type to save the plots as (default: \"png\").","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you prefer to use the Julia API, you can use the benchmark function for generating data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"benchmark(package::Union{PackageSpec,Vector{PackageSpec}}; output_dir::String=\".\", script::Union{String,Nothing}=nothing, tune::Bool=false, exeflags::Cmd=``)\nbenchmark(package_name::String, rev::Union{String,Vector{String}}; output_dir::String=\".\", script::Union{String,Nothing}=nothing, tune::Bool=false, exeflags::Cmd=``)","category":"page"},{"location":"","page":"Home","title":"Home","text":"These output a Dict containing the combined results of the benchmarks, and also output a JSON file in the output_dir for each revision.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"api.md\"]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [AirspeedVelocity]","category":"page"}]
}
